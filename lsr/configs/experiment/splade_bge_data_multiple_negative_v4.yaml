# @package _global_
defaults:
- override /dataset@train_dataset: bge_tuned_v11_2.yaml
- override /loss: multiple_negative_loss

data_collator:
  _target_: lsr.datasets.data_collator.DataCollator

exp_name: splade_bge_data_multiple_negative_v4

loss:
  q_regularizer:
    _target_: lsr.losses.regularizer.FLOPs
    weight: 5e-1
    T: 50000
  d_regularizer:
    T: 50000
    _target_: lsr.losses.regularizer.FLOPs
    weight: 5e-5

model:
  _target_: lsr.models.DualSparseEncoder
  config:
    _target_: lsr.models.DualSparseConfig
    base_model_dir: /home/songzheng/workspace/learned-sparse-retrieval/outputs/splade_msmarco_distil_flops_0.4_0.5/checkpoint-150000/

tokenizer:
  tokenizer_name: distilbert-base-uncased

training_arguments:
  dataloader_drop_last: true
  dataloader_num_workers: 16
  fp16: true
  learning_rate: 5e-6
  max_steps: -1
  num_train_epochs: 10
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  save_steps: 1000
  save_total_limit: 100
  warmup_ratio: 0.1
  warmup_steps: 0
  logging_steps: 100
